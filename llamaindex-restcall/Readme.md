# Get llama3.2 locally
Refer `https://ollama.com/`

# Deploy dependencies
`pip install -r requirements.txt`

# Run  
`start.sh`



